# ===========================================
# IntelliFile - Docker Environment Configuration
# ===========================================
# Copy this file to .env and customize as needed
# Usage: docker compose up -d

# ===========================================
# Application Settings
# ===========================================
APP_NAME=IntelliFile
APP_VERSION=1.0.0
DEBUG=False

# ===========================================
# Ollama LLM Settings
# ===========================================
# Model to use (will be auto-pulled on first run)
OLLAMA_MODEL=llama3.1:latest

# Timeout for LLM requests (seconds)
OLLAMA_TIMEOUT=120

# ===========================================
# Embedding Settings
# ===========================================
# Sentence transformer model for embeddings
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Device for embeddings: cpu or cuda
EMBEDDING_DEVICE=cpu

# ===========================================
# RAG Pipeline Settings
# ===========================================
# Chunk size in characters
CHUNK_SIZE=400

# Overlap between chunks
CHUNK_OVERLAP=50

# Number of results to retrieve
TOP_K_RESULTS=5

# ===========================================
# File Upload Settings
# ===========================================
# Maximum file size in bytes (default: 50MB)
MAX_FILE_SIZE=52428800

# ===========================================
# GPU Support (Optional)
# ===========================================
# To enable GPU for Ollama, uncomment the deploy section
# in docker-compose.yml under the ollama service
#
# To use GPU for embeddings, set:
# EMBEDDING_DEVICE=cuda
