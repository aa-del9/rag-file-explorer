services:
  # ===========================================
  # Ollama - Local LLM Service
  # ===========================================
  ollama:
    image: ollama/ollama:latest
    container_name: intellifile-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    # GPU support (uncomment if you have NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - intellifile-network

  # ===========================================
  # Ollama Model Puller (one-time setup)
  # ===========================================
  ollama-pull:
    image: ollama/ollama:latest
    container_name: intellifile-ollama-pull
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_HOST=http://ollama:11434
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Waiting for Ollama to be ready..."
        sleep 5
        echo "Pulling Llama 3.1 model..."
        ollama pull llama3.1:latest
        echo "Model pull complete!"
    networks:
      - intellifile-network
    restart: "no"

  # ===========================================
  # ChromaDB - Vector Database
  # ===========================================
  chromadb:
    image: chromadb/chroma:latest
    container_name: intellifile-chromadb
    restart: unless-stopped
    ports:
      - "8100:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
      - ALLOW_RESET=TRUE
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/localhost/8000'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - intellifile-network

  # ===========================================
  # IntelliFile Backend - FastAPI Application
  # ===========================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: intellifile-backend
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      # Persistent storage for documents
      - documents_data:/app/data/documents
      # Persistent storage for local ChromaDB (if not using external)
      - backend_chroma_data:/app/data/chroma_db
      # Optional: Mount local documents folder for development
      # - ./backend/data/documents:/app/data/documents
    environment:
      # Application
      - APP_NAME=IntelliFile
      - APP_VERSION=1.0.0
      - DEBUG=${DEBUG:-False}
      # Server
      - HOST=0.0.0.0
      - PORT=8000
      # Ollama LLM
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.1:latest}
      - OLLAMA_TIMEOUT=${OLLAMA_TIMEOUT:-120}
      # Embeddings
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-all-MiniLM-L6-v2}
      - EMBEDDING_DEVICE=${EMBEDDING_DEVICE:-cpu}
      # RAG Settings
      - CHUNK_SIZE=${CHUNK_SIZE:-400}
      - CHUNK_OVERLAP=${CHUNK_OVERLAP:-50}
      - TOP_K_RESULTS=${TOP_K_RESULTS:-5}
      # File Upload
      - MAX_FILE_SIZE=${MAX_FILE_SIZE:-52428800}
    depends_on:
      ollama:
        condition: service_healthy
      chromadb:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - intellifile-network

# ===========================================
# Networks
# ===========================================
networks:
  intellifile-network:
    driver: bridge
    name: intellifile-network

# ===========================================
# Volumes - Persistent Storage
# ===========================================
volumes:
  # Ollama model storage (~4-8GB for Llama 3.1)
  ollama_data:
    name: intellifile-ollama-data
  
  # ChromaDB vector storage
  chroma_data:
    name: intellifile-chroma-data
  
  # Document file storage
  documents_data:
    name: intellifile-documents
  
  # Backend's local ChromaDB (if using embedded mode)
  backend_chroma_data:
    name: intellifile-backend-chroma
